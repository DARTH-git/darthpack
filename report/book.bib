@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.name/knitr/},
}
@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.name/knitr/},
}

@article{Enns2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Enns, E A and Cipriano, L E and Simons, C T and Kong, C Y},
doi = {10.1177/0272989X14528382},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {0272-989X},
journal = {Medical Decision Making},
keywords = {Pareto frontier,calibration,calibration uncertainty,health-economic model,model uncertainty,parameter estimation,parameter uncertainty},
month = {feb},
number = {2},
pages = {170--182},
pmid = {25246403},
title = {{Identifying Best-Fitting Inputs in Health-Economic Model Calibration: A Pareto Frontier Approach}},
url = {http://mdm.sagepub.com/cgi/doi/10.1177/0272989X14528382},
volume = {35},
year = {2015}
}




@article{Menzies2017,
abstract = {PharmacoEconomics, doi:10.1007/s40273-017-0494-4},
author = {Menzies, Nicolas A. and Soeteman, Dj{\o}ra I. and Pandya, Ankur and Kim, Jane J.},
doi = {10.1007/s40273-017-0494-4},
file = {:Users/elinekrijkamp/Library/Application Support/Mendeley Desktop/Downloaded/Menzies et al. - 2017 - Bayesian Methods for Calibrating Health Policy Models A Tutorial.pdf:pdf},
issn = {11792027},
journal = {PharmacoEconomics},
mendeley-groups = {CE15-2018},
number = {6},
pages = {613--624},
pmid = {28247184},
publisher = {Springer International Publishing},
title = {{Bayesian Methods for Calibrating Health Policy Models: A Tutorial}},
volume = {35},
year = {2017}
}




@article{Nelder1965,
    author = {Nelder, JA and Mead, R},
    title = "{A Simplex Method for Function Minimization}",
    journal = {The Computer Journal},
    volume = {7},
    number = {4},
    pages = {308-313},
    year = {1965},
    month = {01},
    abstract = "{A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/7.4.308},
    url = {https://dx.doi.org/10.1093/comjnl/7.4.308},
    eprint = {http://oup.prod.sis.lan/comjnl/article-pdf/7/4/308/1013182/7-4-308.pdf},
}


@article{Alarid-Escudero2018b,
author = {Alarid-Escudero, F and  MacLehose,  RF and Peralta, Y  and Kuntz, KM and Enns EA},
title ={Nonidentifiability in Model Calibration and Implications for Medical Decision Making},
journal = {Medical Decision Making},
volume = {38},
number = {7},
pages = {810-821},
year = {2018},
doi = {10.1177/0272989X18792283},
    note ={PMID: 30248276},
URL = {https://doi.org/10.1177/0272989X18792283},
eprint = {https://doi.org/10.1177/0272989X18792283}
,
    abstract = { Background. Calibration is the process of estimating parameters of a mathematical model by matching model outputs to calibration targets. In the presence of nonidentifiability, multiple parameter sets solve the calibration problem, which may have important implications for decision making. We evaluate the implications of nonidentifiability on the optimal strategy and provide methods to check for nonidentifiability. Methods. We illustrate nonidentifiability by calibrating a 3-state Markov model of cancer relative survival (RS). We performed 2 different calibration exercises: 1) only including RS as a calibration target and 2) adding the ratio between the 2 nondeath states over time as an additional target. We used the Nelder-Mead (NM) algorithm to identify parameter sets that best matched the calibration targets. We used collinearity and likelihood profile analyses to check for nonidentifiability. We then estimated the benefit of a hypothetical treatment in terms of life expectancy gains using different, but equally good-fitting, parameter sets. We also applied collinearity analysis to a realistic model of the natural history of colorectal cancer. Results. When only RS is used as the calibration target, 2 different parameter sets yield similar maximum likelihood values. The high collinearity index and the bimodal likelihood profile on both parameters demonstrated the presence of nonidentifiability. These different, equally good-fitting parameter sets produce different estimates of the treatment effectiveness (0.67 v. 0.31 years), which could influence the optimal decision. By incorporating the additional target, the model becomes identifiable with a collinearity index of 3.5 and a unimodal likelihood profile. Conclusions. In the presence of nonidentifiability, equally likely parameter estimates might yield different conclusions. Checking for the existence of nonidentifiability and its implications should be incorporated into standard model calibration procedures. }
}

@article{Alarid-Escudero2019b,
author = {Alarid-Escudero F, Krijkamp E, Pechlivanoglou P, Jalal H, Kao SY, Yang A, Enns EA},
journal = {PharmacoEconomics},
title = {{A need for change! A coding framework for improving transparency in decision modeling}},
volume = {In press},
year = {2019},
doi = {10.1007/s40273-019-00837-x},
}
@Manual{IMIS,
    title = {IMIS: Increamental Mixture Importance Sampling},
    author = {Adrian Raftery and {Le Bao}},
    year = {2012},
    note = {R package version 0.1},
    url = {https://CRAN.R-project.org/package=IMIS},
  }

@article{Raftery2010,
author = {Raftery, A and Bao, L},
journal = {Biometrics},
number = {4},
pages = {1162--1173},
title = {{Estimating and Projecting Trends in HIV/AIDS Generalized Epidemics Using Incremental Mixture Importance Sampling}},
volume = {66},
year = {2010}
}


@article{Eddy2012,
abstract = {Trust and confidence are critical to the success of health care models. There are two main methods for achieving this: transparency (people can see how the model is built) and validation (how well the model reproduces reality). This report describes recommendations for achieving transparency and validation developed by a taskforce appointed by the International Society for Pharmacoeconomics and Outcomes Research and the Society for Medical Decision Making. Recommendations were developed iteratively by the authors. A nontechnical description - including model type, intended applications, funding sources, structure, intended uses, inputs, outputs, other components that determine function, and their relationships, data sources, validation methods, results, and limitations - should be made available to anyone. Technical documentation, written in sufficient detail to enable a reader with necessary expertise to evaluate the model and potentially reproduce it, should be made available openly or under agreements that protect intellectual property, at the discretion of the modelers. Validation involves face validity (wherein experts evaluate model structure, data sources, assumptions, and results), verification or internal validity (check accuracy of coding), cross validity (comparison of results with other models analyzing the same problem), external validity (comparing model results with real-world results), and predictive validity (comparing model results with prospectively observed events). The last two are the strongest form of validation. Each section of this article contains a number of recommendations that were iterated among the authors, as well as among the wider modeling taskforce, jointly set up by the International Society for Pharmacoeconomics and Outcomes Research and the Society for Medical Decision Making. {\textcopyright} 2012 International Society for Pharmacoeconomics and Outcomes Research (ISPOR).},
annote = {Transparency (people see how the model is built)
Validation (how well the model reproduces reality)

Validation:
- face validity (wherein experts evaluate model structure)
- verification or interna validity (check accuracy of coding)
- cross validity (comparison of results with other models analyzing the same problem)
- external validity (comparing model resutls with prospectively observed events)
- predictive validity (comparing model results with prospective observed events).},
author = {Eddy, David M. and Hollingworth, William and Caro, J. Jaime and Tsevat, Joel and McDonald, Kathryn M. and Wong, John B.},
doi = {10.1177/0272989X12454579},
file = {:Users/elinekrijkamp/Library/Application Support/Mendeley Desktop/Downloaded/Eddy et al. - 2012 - Model transparency and validation A report of the ISPOR-SMDM modeling good research practices task force-7.pdf:pdf},
isbn = {1524-4733},
issn = {0272989X},
journal = {Medical Decision Making},
keywords = {decision sciences,good practices,modeling,simulation,transparency,validation},
mendeley-groups = {CE15-2018},
number = {5},
pages = {733--743},
pmid = {22999134},
title = {{Model transparency and validation: A report of the ISPOR-SMDM modeling good research practices task force-7}},
volume = {32},
year = {2012}
}

@article{Goldhaber_Fiebert2010,
author = {Goldhaber-Fiebert, JD and Stout, NK and Goldie, SJ},
doi = {10.1111/j.1524-4733.2010.00698.x},
isbn = {1098-3015; 1524-4733},
issn = {15244733},
journal = {Value in Health},
keywords = {HPV,cancer,cervical cancer,cost-effectiveness,literature review,methods,microsimulation model,simulation model,validation,validity},
mendeley-tags = {HPV,cervical cancer,literature review,microsimulation model,validity},
number = {5},
pages = {667--674},
pmid = {20230547},
title = {{Empirically evaluating decision-analytic models}},
volume = {13},
year = {2010}
}

@article{Iskandar2018,
author = {Iskandar, R},
doi = {10.1371/journal.pone.0205543},
issn = {1932-6203},
journal = {PloS one},
month = {dec},
number = {12},
pages = {e0205543--e0205543},
publisher = {Public Library of Science},
title = {{A theoretical foundation for state-transition cohort models in health decision analysis}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/30533043 https://www.ncbi.nlm.nih.gov/pmc/PMC6289421/},
volume = {13},
year = {2018}
}


@article{modeest,
    title = {modeest: Mode Estimation},
    author = {Poncet, P},
    year = {2018},
    note = {R package version 2.3.2},
    url = {https://CRAN.R-project.org/package=modeest},
  }

@article{Steele2006,
author = {Steele, R and Raftery, A and Emond, M},
doi = {10.1198/106186006X132358},
file = {:Users/elinekrijkamp/Downloads/Teele, Raftery, Mond - 2006 - Computing Normalizing Constants for Finite Mixture Models via Incremental Mixture Importance Sampling (IMI.pdf:pdf},
journal = {Journal ofComputational and Graphical Statistics},
number = {3},
pages = {712--734},
title = {{Computing Normalizing Constants for Finite Mixture Models via Incremental Mixture Importance Sampling (IMIS)}},
volume = {15},
year = {2006}
}

@article{Rutter2018,
author = {Rutter, C and Ozik, J and DeYoreo, M and Collier N},
journal = {arXiv},
number = {april},
pages = {1--20},
title = {{Microsimulation Model Calibration using Incremental Mixture Approximate Bayesian Computation.}},
url = {https://arxiv.org/abs/1804.02090v3},
year = {2018}
}

@article{Alarid-Escudero2019,
author = {Alarid-Escudero, F and Enns, EA and Kuntz, KM and Michaud, TL and Jalal, H},
file = {:Users/fae/Documents/Mendeley Desktop/Alarid-Escudero et al/Value in Health/Alarid-Escudero et al. - 2019 - Time Traveling Is Just Too Dangerous But Some Methods Are Worth Revisiting The Advantages of Expected Lo.pdf:pdf},
journal = {Value in Health},
keywords = {cost-effectiveness acceptability,effectiveness acceptability curves,enter your own,expected value of perfect,frontier,information},
mendeley-groups = {CEA/PSA},
title = {{"Time Traveling Is Just Too Dangerous" But Some Methods Are Worth Revisiting: The Advantages of Expected Loss Curves Over Cost-Effectiveness Acceptability Curves and Frontier}},
pages = {611-618},
volume = {22},
number = {5},
year = {2019},
doi = {10.1016/j.jval.2019.02.008},
}


@article{Jalal2013,
author = {Jalal, Hawre and Dowd, Bryan and Sainfort, Fran{\c{c}}ois and Kuntz, Karen M.},
doi = {10.1177/0272989X13492014},
file = {:Users/fae/Documents/Mendeley Desktop/Jalal et al/Medical Decision Making/Jalal et al. - 2013 - Linear regression metamodeling as a tool to summarize and present simulation model results.pdf:pdf},
issn = {1552-681X},
journal = {Medical Decision Making},
mendeley-groups = {WorkingPapers/Doulas,Dec Sci/Metamodel,WorkingPapers/DSHR,WorkingPapers/MetamodelR},
number = {7},
pages = {880--90},
pmid = {23811758},
title = {{Linear regression metamodeling as a tool to summarize and present simulation model results.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23811758},
volume = {33},
year = {2013}
}

@incollection{Alarid-Escudero2019d,
author = {Alarid-escudero, Fernando and Gulati, Roman and Rutter, Carolyn M},
booktitle = {Complex Systems and Population Health: A Primer},
editor = {Apostolopoulos, Yorghos and Lich, Kristen Hassmiller and Lemke, Michael K.},
file = {:Users/fae/Documents/Mendeley Desktop/Alarid-escudero, Gulati, Rutter/Complex Systems and Population Health A Primer/Alarid-escudero, Gulati, Rutter - 2019 - Validation of Microsimulation Models Used for Population Health Policy.pdf:pdf},
mendeley-groups = {WorkingPapers/ValidationChapter,Dec Sci/Validation},
pages = {1--13},
publisher = {Oxford University Press},
title = {{Validation of Microsimulation Models Used for Population Health Policy}},
year = {2019}
}
@article{Kopec2010,
abstract = {BACKGROUND: Computer simulation models are used increasingly to support public health research and policy, but questions about their quality persist. The purpose of this article is to review the principles and methods for validation of population-based disease simulation models. METHODS: We developed a comprehensive framework for validating population-based chronic disease simulation models and used this framework in a review of published model validation guidelines. Based on the review, we formulated a set of recommendations for gathering evidence of model credibility. RESULTS: Evidence of model credibility derives from examining: 1) the process of model development, 2) the performance of a model, and 3) the quality of decisions based on the model. Many important issues in model validation are insufficiently addressed by current guidelines. These issues include a detailed evaluation of different data sources, graphical representation of models, computer programming, model calibration, between-model comparisons, sensitivity analysis, and predictive validity. The role of external data in model validation depends on the purpose of the model (e.g., decision analysis versus prediction). More research is needed on the methods of comparing the quality of decisions based on different models. CONCLUSION: As the role of simulation modeling in population health is increasing and models are becoming more complex, there is a need for further improvements in model validation methodology and common standards for evaluating model credibility.},
author = {Kopec, Jacek a and Fin{\`{e}}s, Philippe and Manuel, Douglas G and Buckeridge, David L and Flanagan, William M and Oderkirk, Jillian and Abrahamowicz, Michal and Harper, Samuel and Sharif, Behnam and Okhmatovskaia, Anya and Sayre, Eric C and Rahman, M Mushfiqur and Wolfson, Michael C},
doi = {10.1186/1471-2458-10-710},
file = {:Users/fae/Documents/Mendeley Desktop/Kopec et al/BMC Public Health/Kopec et al. - 2010 - Validation of population-based disease simulation models a review of concepts and methods.pdf:pdf},
issn = {1471-2458},
journal = {BMC Public Health},
keywords = {Chronic Disease,Chronic Disease: epidemiology,Computer Simulation,Computer Simulation: standards,Humans,Models,Public Health,Theoretical,Validation Studies as Topic},
mendeley-groups = {Dec Sci/Validation,InfecDis},
month = {jan},
number = {1},
pages = {710},
pmid = {21087466},
publisher = {BioMed Central Ltd},
title = {{Validation of population-based disease simulation models: a review of concepts and methods.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3001435{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {10},
year = {2010}
}
@article{Vemer2013,
author = {Vemer, Pepijn and Krabbe, P. F.M. and Feenstra, T. L. and {Van Voorn}, G. A.K. and Ramos, Corro and Al, M. J.},
doi = {10.1016/j.jval.2013.06.015},
file = {:Users/fae/Documents/Mendeley Desktop/Vemer et al/Value in Health/Vemer et al. - 2013 - Improving model validation in health technology assessment Comments on guidelines of the ISPOR-SMDM modeling good.pdf:pdf},
issn = {10983015},
journal = {Value in Health},
mendeley-groups = {Dec Sci/Validation},
number = {6},
pages = {1106--1107},
title = {{Improving model validation in health technology assessment: Comments on guidelines of the ISPOR-SMDM modeling good research practices task force}},
volume = {16},
year = {2013}
}

@article{Oberkampf2004,
abstract = {Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V{\&}V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, ie, experimental data, is the issue. This paper presents our viewpoint of the state of the art in V{\&}V in computational physics. (In this paper we refer to all fields of computational engineering and physics, eg, computational fluid dynamics, computational solid mechanics, structural dynamics, shock wave physics, computational chemistry, etc, as computational physics.) We describe our view of the framework in which predictive capability relies on V{\&}V, as well as other factors that affect predictive capability. Our opinions about the research needs and management issues in V{\&}V are very practical: What methods and techniques need to be developed and what changes in the views of management need to occur to increase the usefulness, reliability, and impact of computational physics for decision making about engineering systems? We review the state of the art in V{\&}V over a wide range of topics, for example, prioritization of V{\&}V activities using the Phenomena Identification and Ranking Table (PIRT), code verification, software quality assurance (SQA), numerical error estimation, hierarchical experiments for validation, characteristics of validation experiments, the need to perform nondeterministic computational simulations in comparisons with experimental data, and validation metrics. We then provide an extensive discussion of V{\&}V research and implementation issues that we believe must be addressed for V{\&}V to be more effective in improving confidence in computational predictive capability. Some of the research topics addressed are development of improved procedures for the use of the PIRT for prioritizing V{\&}V activities, the method of manufactured solutions for code verification, development and use of hierarchical validation diagrams, and the construction and use of validation metrics incorporating statistical measures. Some of the implementation topics addressed are the needed management initiatives to better align and team computationalists and experimentalists in conducting validation activities, the perspective of commercial software companies, the key role of analysts and decision makers as code customers, obstacles to the improved effectiveness of V{\&}V, effects of cost and schedule constraints on practical applications in industrial settings, and the role of engineering standards committees in documenting best practices for V{\&}V. There are 207 references cited in this review article.},
author = {Oberkampf, William L and Trucano, Timothy G and Hirsch, Charles},
doi = {10.1115/1.1767847},
file = {:Users/fae/Documents/Mendeley Desktop/Oberkampf, Trucano, Hirsch/Applied Mechanics Reviews/Oberkampf, Trucano, Hirsch - 2004 - Verification, validation, and predictive capability in computational engineering and physics.pdf:pdf},
issn = {00036900},
journal = {Applied Mechanics Reviews},
mendeley-groups = {Dec Sci/Validation},
number = {5},
pages = {345--384},
title = {{Verification, validation, and predictive capability in computational engineering and physics}},
volume = {57},
year = {2004}
}
